
# Online Retail II â€“ Production-Ready ETL

End-to-end, containerized data platform for the UCI **Online Retail II** dataset.  
Built with **PostgreSQL, Airflow, PySpark, ClickHouse, MinIO, Docker**, and ready for **Metabase** dashboards or ML pipelines.

---

## âœ¨ Features
- Raw data lands in **MinIO (S3)** buckets
- **Airflow DAGs** orchestrate ingestion and transformation
- **PySpark** handles data validation and transformations
- **PostgreSQL** serves as staging database
- **ClickHouse** serves as analytics warehouse (star schema)
- **Metabase** for BI dashboards
- **MinIO Parquet** outputs for ML training
- **Docker Compose** for local reproducibility
- Data Quality checks and unit tests included

---

## ğŸ—ï¸ Architecture

1. **Raw landing** â†’ MinIO (`retail-raw/online_retail_ii/*.csv`)
2. **Ingest** â†’ Spark job loads into PostgreSQL staging (`stage.online_retail_ii`)
3. **Transform** â†’ Spark builds star schema â†’ ClickHouse (`dim_*`, `fact_sales`)
4. **Serve** â†’ Metabase connects to ClickHouse; Spark writes parquet features to MinIO (`retail-features`)
5. **Ops** â†’ Airflow DAGs manage orchestration, backfills, and quality checks

---

## ğŸ“‚ Repo Layout

```
online-retail-ii-etl/
â”œâ”€ docker-compose.yml
â”œâ”€ .env.example
â”œâ”€ Makefile
â”œâ”€ README.md
â”œâ”€ airflow/          # Airflow Docker image & DAGs
â”œâ”€ spark/            # Spark jobs for ETL + ML features
â”œâ”€ postgres/         # Staging DB init scripts
â”œâ”€ clickhouse/       # Warehouse DDLs
â”œâ”€ minio/            # Bucket bootstrap script
â”œâ”€ quality/          # Data Quality expectations
â”œâ”€ tests/            # Pytest tests for jobs & schemas
â””â”€ notebooks/        # EDA and ML starter notebook
```

---

## ğŸš€ Quickstart

### 1. Clone & Configure
```bash
git clone https://github.com/your-org/online-retail-ii-etl.git
cd online-retail-ii-etl
cp .env.example .env
```

### 2. Start the stack
```bash
make up
```

### 3. Seed Data
Download the UCI Online Retail II CSV and place under `./data/`. Then run:
```bash
make seed
```

### 4. Access Services
- **Airflow** â†’ http://localhost:8080  (admin/admin)
- **MinIO Console** â†’ http://localhost:9001  (admin/adminadmin)
- **Metabase** â†’ http://localhost:3000

---

## ğŸ—„ï¸ Warehouse Schema (ClickHouse)

- **dim_customer(CustomerID, customer_country)**
- **dim_product(StockCode, product_desc)**
- **dim_date(date)**
- **fact_sales(InvoiceNo, CustomerID, StockCode, date, Quantity, UnitPrice, amount)**  
  Partitioned monthly by `date`.

Materialized View:
- **mv_daily_revenue(date, revenue)**

---

## ğŸ§ª Tests
Run unit tests:
```bash
pytest tests/
```

---

## ğŸ”’ Production Notes
- Switch Airflow executor to Celery/K8s for scale
- Use external object storage (AWS S3, GCS, etc.)
- Secure secrets with Vault/SSM (donâ€™t commit `.env`)
- Monitor Airflow, Spark, and ClickHouse with Prometheus
- Expand Data Quality checks (Great Expectations)

---

## ğŸ“Š Next Steps
- Build Metabase dashboards (Revenue trends, RFM segments, Cohorts)
- Generate ML features via Spark jobs (stored in MinIO Parquet)
- Extend schema with SCDs, embeddings, or vector search in ClickHouse

---

## ğŸ“– Dataset License
The **Online Retail II** dataset is provided by UCI ML Repository. Review license/terms before production use.

---

## ğŸ™Œ Acknowledgements
- Apache Airflow, Apache Spark, PostgreSQL, ClickHouse, MinIO, Docker, Metabase
